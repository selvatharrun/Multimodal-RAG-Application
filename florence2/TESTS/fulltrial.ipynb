{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "from transformers import pipeline\n",
    "import base64\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import pdfplumber  # For table extraction from PDFs\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "import base64\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pytesseract for OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\sselva\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"qwen2.5:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform OCR on images\n",
    "def perform_ocr(image):\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "# Function to process PDF files and convert them into markdown\n",
    "def process_pdf(file_path):\n",
    "    pdf_content = []\n",
    "    with fitz.open(file_path) as pdf_document:\n",
    "        for page_number in range(len(pdf_document)):\n",
    "            page = pdf_document[page_number]\n",
    "            text = page.get_text(\"text\")\n",
    "            pdf_content.append({'type': 'paragraph', 'text': text, 'page_number': page_number + 1})\n",
    "\n",
    "            image_list = page.get_images(full=True)\n",
    "            for img in image_list:\n",
    "                xref = img[0]\n",
    "                base_image = pdf_document.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image_stream = io.BytesIO(image_bytes)\n",
    "                image = Image.open(image_stream)\n",
    "\n",
    "                # OCR the image\n",
    "                ocr_text = pytesseract.image_to_string(image)\n",
    "                pdf_content.append({'type': 'image', 'text': ocr_text, 'page_number': page_number + 1})\n",
    "\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page_number, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                pdf_content.append({'type': 'table', 'data': table, 'page_number': page_number + 1})\n",
    "\n",
    "    return pdf_content\n",
    "\n",
    "# Convert extracted content to markdown\n",
    "def convert_to_markdown(content):\n",
    "    if content['type'] == 'paragraph':\n",
    "        return f\"{content.get('text', '')}\\n\\n\"\n",
    "    elif content['type'] == 'table':\n",
    "        table_md = \"\"\n",
    "        for i, row in enumerate(content['data']):  # Convert table data to markdown format\n",
    "            table_md += \"| \" + \" | \".join(row) + \" |\\n\"\n",
    "            if i == 0:\n",
    "                table_md += \"|\" + \"|\".join([\"---\" for _ in row]) + \"|\\n\"\n",
    "        return table_md + \"\\n\"\n",
    "    elif content['type'] == 'image':\n",
    "        return f\"[Image OCR: {content.get('text', '')}]\\n\\n\"\n",
    "    return \"\"\n",
    "\n",
    "# Custom function to load a string as a Document object\n",
    "def load_from_string(text: str):\n",
    "    document = Document(page_content=text, metadata={\"source\": \"string_input\"})\n",
    "    return [document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_documents_with_qdrant(docs, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    # Split the document into chunks based on topic structure\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Embedding with HuggingFace model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name)\n",
    "    # Qdrant for in-memory vector storage\n",
    "    qdrant = Qdrant.from_embeddings(split_docs,embedding_model, location=\":memory:\", collection_name=\"my_documents\")\n",
    "    \n",
    "    return qdrant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"C:\\\\Users\\\\sselva\\\\Downloads\\\\florence2\\\\sama_test 1\\\\testddoc1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = process_pdf(file_path)\n",
    "\n",
    "# Convert to markdown\n",
    "markdown_text = \"\"\n",
    "for item in content:\n",
    "    markdown_text += convert_to_markdown(item)\n",
    "\n",
    "# Write markdown to file\n",
    "markdown_file = file_path.replace(\".pdf\", \".md\")\n",
    "with open(markdown_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_from_string(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple ONNX files found in 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2': ['onnx/model.onnx', 'onnx/model_O1.onnx', 'onnx/model_O2.onnx', 'onnx/model_O3.onnx', 'onnx/model_O4.onnx', 'onnx/model_qint8_arm64.onnx', 'onnx/model_qint8_avx512.onnx', 'onnx/model_qint8_avx512_vnni.onnx', 'onnx/model_quint8_avx2.onnx'], defaulting to 'onnx/model.onnx'. Please specify the desired file name via `model_kwargs={\"file_name\": \"<file_name>\"}`.\n",
      "c:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Qdrant\u001b[38;5;241m=\u001b[39m\u001b[43mprocess_documents_with_qdrant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m, in \u001b[0;36mprocess_documents_with_qdrant\u001b[1;34m(docs, model_name)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Embedding with HuggingFace model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_name, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m embeddings\u001b[38;5;241m=\u001b[39m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Qdrant for in-memory vector storage\u001b[39;00m\n\u001b[0;32m     15\u001b[0m qdrant \u001b[38;5;241m=\u001b[39m Qdrant\u001b[38;5;241m.\u001b[39mfrom_embeddings(embeddings, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:memory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:589\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    588\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 589\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1044\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m            \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:386\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    384\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 386\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    387\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    388\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "Qdrant=process_documents_with_qdrant(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what is tesseract?\"\n",
    "def qdrant_search(query,Qdrant):\n",
    "    search_results = Qdrant.similarity_search_with_score(query)\n",
    "    return [(doc.page_content, score) for doc, score in search_results]\n",
    "doc_context = qdrant_search(query,Qdrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[Image OCR: ? Tesseract at UB Mannheim\\n\\nThe Mannheim University Library (UB Mannheim) uses Tesseract to perform text recognition (OCR = optical character\\nrecognition) for historical German newspapers (Allgemeine PreuBische Staatszeitung, Deutscher Reichsanzeiger). The latest\\nresults with text from more than 700000 pages are available online.\\n\\nTesseract installer for Windows', 0.3482387907556797), ('‘The QCD sum rules (QCDSR) is one of the most power-\\nful non-perturbative approach, and has been widely used to\\nanalyze the mass spectra and the decay behavior of hadrons,\\n[35-50]. In recent years, some tasks were carried out by three-\\npoint QCDSR, such as the analysis of electroweak and elec-\\n‘tromagnetic form factors 12, 51-57], and the strong coupling\\nconstants [58-68]. These parameters are very important to\\nanalyze the decay process of hadrons. In our previous work,', 0.22397772019376147), (']', 0.2069307255669893), ('(eG), (FG) and (q)(e2G?) are considered in QCD side.\\nSec. IV is employed to present the numerical results and dis-\\ncussions. Sec. Vis reserved as conclusions. Some important\\nfigures are shown in Appendix.', 0.20497241487159085)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'string_input', '_id': '9615e707968c4f288fcdbcc64fee8609', '_collection_name': 'my_documents'}, page_content='“The layout of this paper is as follows, After introduction in\\nSec. I, the radiative decays of the vector heavy-light mesons\\nare analyzed inthe framework of SM in Sec. I, and the elec-\\ntromagnetic form factor is introduced. In Sec. II, we sys-\\ntematically analyze the electromagnetic form factors of vec-\\ntor heavy-light meson to pseudoscalar heavy-light meson by\\nthe three-point QCDSR, where the contributions of perturba-\\ntive part and vacuum condensate including (Gq), @g.Gq),'), Document(metadata={'source': 'string_input', '_id': 'd6ede898583349148fc4e5d87fbd427c', '_collection_name': 'my_documents'}, page_content='(eG), (FG) and (q)(e2G?) are considered in QCD side.\\nSec. IV is employed to present the numerical results and dis-\\ncussions. Sec. Vis reserved as conclusions. Some important\\nfigures are shown in Appendix.'), Document(metadata={'source': 'string_input', '_id': '17d6d99dae28451fb87cf53458ccbc72', '_collection_name': 'my_documents'}, page_content='‘The QCD sum rules (QCDSR) is one of the most power-\\nful non-perturbative approach, and has been widely used to\\nanalyze the mass spectra and the decay behavior of hadrons,\\n[35-50]. In recent years, some tasks were carried out by three-\\npoint QCDSR, such as the analysis of electroweak and elec-\\n‘tromagnetic form factors 12, 51-57], and the strong coupling\\nconstants [58-68]. These parameters are very important to\\nanalyze the decay process of hadrons. In our previous work,'), Document(metadata={'source': 'string_input', '_id': '425966802af54fa98bfecc6a890b4ba5', '_collection_name': 'my_documents'}, page_content='[Image OCR: ? Tesseract at UB Mannheim\\n\\nThe Mannheim University Library (UB Mannheim) uses Tesseract to perform text recognition (OCR = optical character\\nrecognition) for historical German newspapers (Allgemeine PreuBische Staatszeitung, Deutscher Reichsanzeiger). The latest\\nresults with text from more than 700000 pages are available online.\\n\\nTesseract installer for Windows')]\n"
     ]
    }
   ],
   "source": [
    "prompt=\"can you summarize the different topics of the pdf for me?\"\n",
    "doc_context = Qdrant.similarity_search(prompt)[0:3]\n",
    "print(doc_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = f\"Document context:\\n{doc_context}\\n\\nQuestion: {prompt}\"\n",
    "response = llm(structured_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split the document into chunks based on topic structure\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \",\".\", \"\",]\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "# Assuming 'docs' is a list of Document objects\n",
    "corpus = [{'id': i, 'metadata': doc.metadata, 'text': doc.page_content} for i, doc in enumerate(split_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "import Stemmer\n",
    "\n",
    "def init_ret(docs):\n",
    "    # Assuming 'docs' is a list of Document objects\n",
    "    corpus = [{'id': i, 'metadata': doc.metadata, 'text': doc.page_content} for i, doc in enumerate(docs)]\n",
    "    # optional: create a stemmer\n",
    "    stemmer = Stemmer.Stemmer(\"english\")\n",
    "    # Extracting just the text from the corpus for tokenization\n",
    "    texts = [doc['text'] for doc in corpus]  # Assuming 'corpus' is a list of dictionaries\n",
    "\n",
    "    # Now pass the extracted texts to the tokenize function\n",
    "    corpus_tokens = bm25s.tokenize(texts, stopwords=\"en\", stemmer=stemmer)\n",
    "\n",
    "    # Create the BM25 model and index the corpus\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    return retriever,stemmer\n",
    "\n",
    "retreiver,stemmer = init_ret(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query: Tokenized(ids=[[1, 0]], vocab={'tesseract': 0, 'what': 1})\n",
      "Adjusted k: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sselva\\AppData\\Local\\Temp\\ipykernel_9856\\1109737116.py:18: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm(structured_prompt)\n"
     ]
    }
   ],
   "source": [
    "query = \"what is tesseract?\"\n",
    "def bm25ans(query, retriever, stemmer, corpus):\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    print(f\"Tokenized query: {query_tokens}\")\n",
    "    \n",
    "    # Adjust k based on the number of query tokens\n",
    "    k = min(2, len(query_tokens[0]))  # Ensure k is <= number of tokens\n",
    "    print(f\"Adjusted k: {k}\")\n",
    "    \n",
    "    # Retrieve top-k results\n",
    "    results = retriever.retrieve(query_tokens, corpus=corpus, k=k)\n",
    "    return results\n",
    "\n",
    "doc_con=bm25ans(query,retreiver,stemmer,corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query: Tokenized(ids=[[1, 0]], vocab={'tesseract': 0, 'what': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "def bm25s_search(query, retriever, stemmer, corpus):\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    print(f\"Tokenized query: {query_tokens}\")\n",
    "    \n",
    "    # Adjust k based on the number of query tokens\n",
    "    k = min(2, len(query_tokens[0]))  # Ensure k is <= number of tokens\n",
    "    \n",
    "    # Retrieve top-k results\n",
    "    results,scores = retriever.retrieve(query_tokens, corpus=corpus, k=k)\n",
    "    \n",
    "    # Combine the results with the scores\n",
    "    scored_results = [(results[i][i]['text'], scores[i][i]) for i in range(len(results))]\n",
    "    \n",
    "    return scored_results\n",
    "\n",
    "re = bm25s_search(query,retreiver,stemmer,corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from typing import List, Tuple, Dict, Any\n",
    "# Hybrid Search - Reciprocal Rank Fusion\n",
    "class ReciprocalRankFusion:\n",
    "    def __init__(self, k: float = 60.0):\n",
    "        self.k = k\n",
    "\n",
    "    def fuse(self, ranked_lists, top_n: int = 3):\n",
    "        item_ranks = {}\n",
    "        for lst in ranked_lists:\n",
    "            for rank, (item, score) in enumerate(lst, start=1):\n",
    "                if item not in item_ranks:\n",
    "                    item_ranks[item] = [len(ranked_lists) + 1] * len(ranked_lists)\n",
    "                item_ranks[item][ranked_lists.index(lst)] = rank\n",
    "\n",
    "        fused_scores = []\n",
    "        for item, ranks in item_ranks.items():\n",
    "            fused_score = sum(1 / (rank + self.k) for rank in ranks)\n",
    "            heapq.heappush(fused_scores, (-fused_score, item))\n",
    "\n",
    "        # Return top-n results\n",
    "        return [(item, -score) for score, item in sorted(fused_scores, reverse=True)[:top_n]]\n",
    "\n",
    "# Fuse results using Reciprocal Rank Fusion\n",
    "rrf = ReciprocalRankFusion()\n",
    "doc_context = rrf.fuse([re,ans], top_n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(']', 0.031746031746031744), ('WARNING: Tesseract should be either installed in the directory which is suggested during the installation or in a new\\ndirectory. The uninstaller removes the whole installation directory. If you installed Tesseract in an existing directory, that\\ndirectory will be removed with all its subdirectories and files.\\n\\nThe latest installers can be downloaded here:\\n\\n© tesseract-ocr-w64-setup-5.4.0.20240606.exe (64 bit)\\n\\nThere are also older versions for 32 and 64 bit Windows available.', 0.03200204813108039), ('[Image OCR: ? Tesseract at UB Mannheim\\n\\nThe Mannheim University Library (UB Mannheim) uses Tesseract to perform text recognition (OCR = optical character\\nrecognition) for historical German newspapers (Allgemeine PreuBische Staatszeitung, Deutscher Reichsanzeiger). The latest\\nresults with text from more than 700000 pages are available online.\\n\\nTesseract installer for Windows', 0.032018442622950824)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = f\"Document context:\\n{doc_context}\\n\\nQuestion: {query}\"\n",
    "response = llm(structured_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ko(doc_context, prompt):\n",
    "    ko_prompt = f\"\"\"\n",
    "    You are an AI assistant tasked with generating a Knowledge Object based on the given context and user input.\n",
    "    Context: '{doc_context}'\n",
    "    Use this context to generate a detailed KO in the following format:\n",
    "    \n",
    "    - Short Description: (Explain the root cause of the problem)\n",
    "    - Symptoms: (List observable signs or behaviors indicating the issue)\n",
    "    - Long Description: (Provide a detailed description of the problem or issue in 50 words)\n",
    "    - Causes: (Identify the factors that led to this issue)\n",
    "    - Resolution Note: (Give a step-by-step resolution for the problem, covering all scenarios)\n",
    "    \n",
    "    Question: '{prompt}'\n",
    "    \"\"\"\n",
    "    response = st.session_state.llm(ko_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_knowledge_object(doc_context: str, prompt: str) -> str:\n",
    "    # Dummy function to simulate generating a KO article\n",
    "    return f\"\"\"\n",
    "    You are an AI assistant tasked with generating a Knowledge Object based on the given context and user input.\n",
    "    Context: '{doc_context}'\n",
    "    Use this context to generate a detailed KO in the following format:\n",
    "    \n",
    "    - Short Description: (Explain the root cause of the problem)\n",
    "    - Symptoms: (List observable signs or behaviors indicating the issue)\n",
    "    - Long Description: (Provide a detailed description of the problem or issue in 50 words)\n",
    "    - Causes: (Identify the factors that led to this issue)\n",
    "    - Resolution Note: (Give a step-by-step resolution for the problem, covering all scenarios)\n",
    "    \n",
    "    Question: '{prompt}'\n",
    "    \"\"\"\n",
    "\n",
    "def run(model: str, doc_context: str, question: str):\n",
    "    client = ollama.Client()\n",
    "\n",
    "    # Initialize conversation with a user query\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "    # First API call: Send the query and function description to the model\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"generate_knowledge_object\",\n",
    "                    \"description\": \"Generate a Knowledge Object based on context and user input\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"doc_context\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Document context relevant to the KO generation\"\n",
    "                            },\n",
    "                            \"prompt\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The user's question or input for generating the KO\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"doc_context\", \"prompt\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Add the model's response to the conversation history\n",
    "    messages.append(response[\"message\"])\n",
    "\n",
    "    # Check if the model decided to use the provided function\n",
    "    if not response[\"message\"].get(\"tool_calls\"):\n",
    "        print(\"The model didn't use the function. Its response was:\")\n",
    "        print(response[\"message\"][\"content\"])\n",
    "        return\n",
    "\n",
    "    # Process function calls made by the model\n",
    "    if response[\"message\"].get(\"tool_calls\"):\n",
    "        available_functions = {\n",
    "            \"generate_knowledge_object\": generate_knowledge_object,\n",
    "        }\n",
    "\n",
    "        for tool in response[\"message\"][\"tool_calls\"]:\n",
    "            function_to_call = available_functions[tool[\"function\"][\"name\"]]\n",
    "            function_args = tool[\"function\"][\"arguments\"]\n",
    "\n",
    "            # Ensure the necessary arguments are included\n",
    "            if \"doc_context\" not in function_args or \"prompt\" not in function_args:\n",
    "                print(\"Missing required arguments for the function call.\")\n",
    "                return\n",
    "\n",
    "            # Call the function to generate the KO article\n",
    "            function_response = function_to_call(doc_context=doc_context, prompt=function_args[\"prompt\"])\n",
    "\n",
    "            # Add function response to the conversation\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Second API call: Get final response from the model\n",
    "    final_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "    print(final_response[\"message\"][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[Image OCR: ? Tesseract at UB Mannheim\\n\\nThe Mannheim University Library (UB Mannheim) uses Tesseract to perform text recognition (OCR = optical character\\nrecognition) for historical German newspapers (Allgemeine PreuBische Staatszeitung, Deutscher Reichsanzeiger). The latest\\nresults with text from more than 700000 pages are available online.\\n\\nTesseract installer for Windows', 0.3482387907556797), ('‘The QCD sum rules (QCDSR) is one of the most power-\\nful non-perturbative approach, and has been widely used to\\nanalyze the mass spectra and the decay behavior of hadrons,\\n[35-50]. In recent years, some tasks were carried out by three-\\npoint QCDSR, such as the analysis of electroweak and elec-\\n‘tromagnetic form factors 12, 51-57], and the strong coupling\\nconstants [58-68]. These parameters are very important to\\nanalyze the decay process of hadrons. In our previous work,', 0.22397772019376147), (']', 0.2069307255669893), ('(eG), (FG) and (q)(e2G?) are considered in QCD side.\\nSec. IV is employed to present the numerical results and dis-\\ncussions. Sec. Vis reserved as conclusions. Some important\\nfigures are shown in Appendix.', 0.20497241487159085)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Short Description:\n",
      "Tesseract is an optical character recognition (OCR) library designed for use with Google's Cloud Vision API. It's widely used in various applications to convert images into text.\n",
      "\n",
      "### Symptoms:\n",
      "- Users are unable to install or run Tesseract on their local machines.\n",
      "- Attempting to access the latest results of Tesseract fails due to connection issues, leading to a 403 Forbidden error (as per user feedback).\n",
      "- The installation process for Windows is not clear and requires extensive manual steps.\n",
      "\n",
      "### Long Description:\n",
      "Tesseract, developed by Google, is an OCR library that aims to recognize text from images. It’s crucial in fields such as document digitization and image processing where the ability to interpret scanned documents or images into readable text is essential. However, users often face difficulties with installing Tesseract due to several reasons. The installation process involves downloading a ZIP file containing multiple components (such as libraries and dependencies), extracting it, and then running specific scripts for each component. This can be complicated and error-prone, leading to frustration among users who are not familiar with the technical processes involved.\n",
      "\n",
      "Additionally, the latest results of Tesseract are often inaccessible or require external resources that may not always be available, causing delays in data availability. Users report issues with installation scripts failing due to syntax errors or missing dependencies, which can lead to a 403 Forbidden error when trying to access the online repositories for the most recent versions.\n",
      "\n",
      "### Causes:\n",
      "1. **Incorrect Installation Process**: The documentation provided is often not clear and lacks detailed instructions on how to install Tesseract correctly.\n",
      "2. **Dependencies Issues**: Missing or incompatible libraries may prevent certain components from being properly installed, leading to errors during the installation process.\n",
      "3. **Network Connectivity Problems**: External dependencies required for running the scripts are either missing or unreliable due to network issues, causing failed installations.\n",
      "\n",
      "### Resolution Note:\n",
      "1. **Clear Documentation**: Ensure that the documentation provided with Tesseract is comprehensive and easy to understand. Include clear instructions on how to install prerequisites like Python, libraries (like PIL), etc.\n",
      "2. **Automated Scripts**: Develop more automated installation scripts or use Docker containers to simplify the installation process. This would streamline the installation procedure for users without prior experience in software development.\n",
      "3. **Backup Mechanism**: Provide a backup mechanism that can be used if the primary repository is unavailable, ensuring continuous access to the latest versions even when network connectivity issues arise.\n",
      "\n",
      "By addressing these points, Tesseract should become more accessible and usable for developers and end-users alike, improving their overall experience with this powerful OCR tool.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = \"qwen2.5:1.5b\"\n",
    "    doc_context = doc_context\n",
    "    question = \"generate a KO article on tesseract\"\n",
    "    run(model, doc_context, question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
