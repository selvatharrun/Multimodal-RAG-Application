{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from API.sama_updated import read_docx, process_pdf, convert_to_markdown, extract_text_from_pptx\n",
    "from pages.imports.searchmethods import qdrant_search,ReciprocalRankFusion,bm25s_search,KO\n",
    "from API.open import process\n",
    "from API.florence import process_file_with_florence\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "import tempfile\n",
    "import bm25s    \n",
    "import Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_string(text: str):\n",
    "    document = Document(page_content=text, metadata={\"source\": \"string_input\"})\n",
    "    return [document]\n",
    "\n",
    "def process_documents_with_qdrant(docs, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    qdrant = Qdrant.from_documents(split_docs, embedding_model, location=\":memory:\", collection_name=\"my_documents\")\n",
    "    return qdrant\n",
    "\n",
    "def init_bm25s_retriever(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \",\".\", \"\",]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    corpus = [{'id': i, 'metadata': doc.metadata, 'text': doc.page_content} for i, doc in enumerate(split_docs)]\n",
    "    stemmer = Stemmer.Stemmer(\"english\")\n",
    "    texts = [doc['text'] for doc in corpus]\n",
    "    corpus_tokens = bm25s.tokenize(texts, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    return retriever, corpus, stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'KS100121_Modify eGroup Approver.docx', 'response': [('**Modify eGroup Approver/** [Size: 304800]\\n\\n**Administrator/Reviewer** [Size: 304800]\\n\\n \\n\\nCheck approval matrix sheet so that we come to know whose approval is required [Color: 201F1E][Size: 139700]\\n\\nCheck the type of eGroup [Color: 201F1E][Size: 139700]\\n\\nCheck for the same in approval matrix sheet so as to whose permission is required [Color: 201F1E][Size: 139700]', 1.260368)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def chatbot_api(\n",
    "    file_path,  # Direct file path\n",
    "    ocr_method,\n",
    "    search_method,\n",
    "    prompt\n",
    "):\n",
    "    uploads_folder = \"uploads\"\n",
    "    os.makedirs(uploads_folder, exist_ok=True)\n",
    "    \n",
    "    # Copy the file to uploads folder (optional step if you want to keep a local copy)\n",
    "    file_dest_path = os.path.join(uploads_folder, os.path.basename(file_path))\n",
    "    \n",
    "    try:\n",
    "        shutil.copy(file_path, file_dest_path)  # Copy file to destination path\n",
    "    except FileNotFoundError:\n",
    "        return {\"message\": f\"File not found at the given path: {file_path}\"}\n",
    "\n",
    "    # Verify if the file was successfully copied\n",
    "    if not os.path.exists(file_dest_path):\n",
    "        return {\"message\": f\"Failed to copy file to destination path: {file_dest_path}\"}\n",
    "\n",
    "    # Extract content based on file type\n",
    "    if file_dest_path.endswith(\".pdf\"):\n",
    "        content = process_pdf(file_dest_path)\n",
    "    elif file_dest_path.endswith(\".docx\"):\n",
    "        content = read_docx(file_dest_path)\n",
    "    else:\n",
    "        return {\"message\": \"Unsupported file format\"}\n",
    "\n",
    "    # OCR processing\n",
    "    if ocr_method == \"openai\":\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        all_text = process(filename=file_dest_path, api_key=api_key, verbose=True, cleanup=True)\n",
    "    elif ocr_method == \"florence\":\n",
    "        output_folder = os.path.join(uploads_folder, \"florence_output\")\n",
    "        ocr_results = process_file_with_florence(file_dest_path, output_folder, verbose=True)\n",
    "        all_text = \" \".join(ocr_results)\n",
    "    else:  # Default to Tesseract\n",
    "        all_text = \"\"\n",
    "        for item in content:\n",
    "            all_text += convert_to_markdown(item)\n",
    "\n",
    "    # Initialize search method\n",
    "    doc_texts = load_from_string(all_text)\n",
    "    qdrant = process_documents_with_qdrant(doc_texts) if search_method == \"Embedding + Qdrant\" else None\n",
    "    bm25_retriever, bm25_corpus, bm25_stemmer = init_bm25s_retriever(doc_texts) if search_method == \"BM25S\" else (None, None, None)\n",
    "\n",
    "    # Get document context using search method\n",
    "    if search_method == \"Embedding + Qdrant\":\n",
    "        doc_context = qdrant_search(prompt, qdrant)\n",
    "    elif search_method == \"BM25S\":\n",
    "        doc_context = bm25s_search(prompt, bm25_retriever, bm25_stemmer, bm25_corpus)\n",
    "    else:\n",
    "        return {\"message\": \"Unsupported search method.\"}\n",
    "\n",
    "    return {\"filename\": os.path.basename(file_path), \"response\": doc_context}\n",
    "\n",
    "# Example usage in Jupyter Notebook\n",
    "result = chatbot_api(\n",
    "    file_path=\"C:\\\\Users\\\\sselva\\\\Downloads\\\\KO Documents\\\\KO Documents\\\\KS100121_Modify eGroup Approver.docx\",\n",
    "    ocr_method=\"tesseract\",\n",
    "    search_method=\"BM25S\",\n",
    "    prompt=\"how to approve modify egroup request?\"\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchatbot_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43msselva\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mKO Documents\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mKO Documents\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mKS100121_Modify eGroup Approver.docx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mocr_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesseract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msearch_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBM25S\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhow to approve modify egroup request?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mchatbot_api\u001b[1;34m(file, ocr_method, search_method, prompt)\u001b[0m\n\u001b[0;32m      9\u001b[0m uploads_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muploads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(uploads_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(uploads_folder, \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     14\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(file\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'filename'"
     ]
    }
   ],
   "source": [
    "chatbot_api(file=\"C:\\\\Users\\\\sselva\\\\Downloads\\\\KO Documents\\\\KO Documents\\\\KS100121_Modify eGroup Approver.docx\",ocr_method=\"tesseract\",search_method=\"BM25S\",prompt=\"how to approve modify egroup request?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = 'AIzaSyAIWLIxKL1yCk5Skw1WzNZHvPRyK5jhd6g'\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI language model, so I don't have feelings or experiences like humans do. But I am here and ready to assist you with any questions or tasks you may have! How can I help you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "h=llm.invoke(\"hi how are you?\")\n",
    "print(h.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Cleaned up temporary files.\n"
     ]
    }
   ],
   "source": [
    "from API.florence import process_file_with_florence\n",
    "ocr_results = process_file_with_florence(file_path=\"C:\\\\Users\\\\sselva\\\\Downloads\\\\KO Documents\\\\KO Documents\\\\KS100121_Modify eGroup Approver.docx\", output_folder=\"uploads\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify eGroup Approver/\n",
      "Administrator/Reviewer\n",
      "\n",
      "Check approval matrix sheet so that we come to know whose approval is required\n",
      "Check the type of eGroup\n",
      "Check for the same in approval matrix sheet so as to whose permission is required\n",
      "Navigate to bst.golder.com  eAdministration  Setup  employee group setup  eGroup Type (eTime or eExpense and Supervisor or Administrator)\n",
      "Find name of that eGroup \n",
      "Click on eGroup name and select it\n",
      "Information will be shown at right side in that update name of Primary or Alternate 1 or Alternate 2 or Alternate 3 as per requirement \n",
      "Click on ‘save’ icon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ocr_results[0].get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract at UB Mannheim\n",
      "The Mannheim University Library (UB Mannheim) uses Tesseract to perform text recognition (OCR = optical character\n",
      "recognition) for historical German newspapers (Allgemeine Preußische Staatszeitung , Deutscher Reichsanzeiger). The latest\n",
      "results with text from more than 700000 pages are available online .\n",
      "Tesseract installer for Windows\n",
      "Normally we run Tesseract on Debian GNU Linux , but there was also the need for a Windows version . That 's why we have built\n",
      "a Tesseract installer for Windows .\n",
      "WARNING : Tesseract should be either installed in the directory which is suggested during the installation or in a new\n",
      "directory . The uninstaller removes the whole installation directory . If you installed Tesseract in an existing directory , that\n",
      "directory will be removed with all its subdirectories and files .\n",
      "The latest installers can be downloaded here :\n",
      "• tesseract - ocr - w64 - setup - 5.4.0.20240606.exe ( 64 bit )\n",
      "There are also older versions for 32 and 64 bit Windows available .\n",
      "In addition , we also provide documentation which was generated by Doxygen .\n",
      "SUBJECT IN FOCUS : Origin of the severe acute respiratory syndrome coronavirus - 2\n",
      "( SARS - CoV - 2 ) , the virus causing COVID - 19\n",
      "The first human cases of COVID - 19 , the disease caused by the novel coronavirus causing COVID - 19 , subsequently\n",
      "named SARS - CoV - 2 were first reported by officials in Wuhan City , China , in December 2019 . Retrospective\n",
      "investigations by Chinese authorities have identified human cases with onset of symptoms in early December 2019 .\n",
      "While some of the earliest known cases had a link to a wholesale food market in Wuhan , some did not . Many of the\n",
      "initial patients were either stall owners , market employees , or regular visitors to this market . Environmental samples\n",
      "taken from this market in December 2019 tested positive for SARS - CoV - 2 , further suggesting that the market in\n",
      "Wuhan City was the source of this outbreak or played a role in the initial amplification of the outbreak . The market\n",
      "was closed on 1 January 2020 .\n",
      "SARS - CoV - 2 was identified in early January and its genetic sequence shared publicly on 11 - 12 January . The full\n",
      "genetic sequence of SARS - CoV - 2 from the early human cases and the sequences of many other virus isolated from\n",
      "human cases from China and all over the world since then show that SARS - CoV - 2 has an ecological origin in bat\n",
      "populations . All available evidence to date suggests that the virus has a natural animal origin and is not a\n",
      "manipulated or constructed virus . Many researchers have been able to look at the genomic features of SARS - CoV - 2\n",
      "and have found that evidence does not support that SARS - CoV - 2 is a laboratory construct . If it were a constructed\n",
      "virus , its genomic sequence would show a mix of known elements . This is not the case .\n",
      "I . INTRODUCTION\n",
      "As one of the most interesting research areas in particle\n",
      "physics , the experimental and theoretical investigations about\n",
      "the heavy flavor hadrons have been developing rapidly . This\n",
      "is due to their key roles in both understanding of QCD long -\n",
      "distance dynamics , and the determination of the fundamental\n",
      "parameters of the standard model ( SM ) . Especially , the decay\n",
      "behavior of heavy flavor hadrons is an important and interest -\n",
      "ing research topic for testing the SM and finding new physics\n",
      "beyond the SM . These decay processes can be classified into\n",
      "leptonic , semileptonic and nonleptonic decays which involve\n",
      "both electroweak and strong interactions . However , theoret -\n",
      "ical investigations about the heavy hadrons is very difficult\n",
      "because the QCD is non - perturbative in low energy regions .\n",
      "Therefore , many phenomenological approaches [ 1 - 34 ] have\n",
      "been employed to analyze the decay processes of the heavy\n",
      "flavor hadrons .\n",
      "The QCD sum rules ( QCDSR ) is one of the most power -\n",
      "ful non - perturbative approach , and has been widely used to\n",
      "analyze the mass spectra and the decay behavior of hadrons\n",
      "[ 35 - 50 ] . In recent years , some tasks were carried out by three -\n",
      "point QCDSR , such as the analysis of electroweak and elec -\n",
      "tromagnetic form factors [ 12 , 51 - 57 ] , and the strong coupling\n",
      "constants [ 58 - 68 ] . These parameters are very important to\n",
      "analyze the decay process of hadrons . In our previous work ,\n",
      "the B B electromagnetic form factor were studied by\n",
      "three - point QCDSR , and the corresponding radiative decay\n",
      "B Bey was calculated [ 69 ] . In Ref [ 70 ] , we analyzed the\n",
      "momentum dependent strong coupling constant GDED , ( Q2 ) .\n",
      "According vector meson dominant ( VMD ) , the electromag -\n",
      "netic coupling constant GDD₂y was also obtained by setting\n",
      "Q20 . And the radiative decay width for D → Day\n",
      "heavy - light mesons . Based on these results , the decay widths\n",
      "about vector heavy - light mesons to pseudoscalar heavy - light\n",
      "mesons plus a photon are obtained . Although some radiative\n",
      "decays , such as D*+ → D⁺y , D0 Dºy and D→ D₂y were\n",
      "already analyzed by other research groups [ 21 - 24 , 51 ] . How -\n",
      "ever , these results were obtained by different methods and not\n",
      "consistent well with each other , which needs further confirma -\n",
      "tion by other theoretical methods .\n",
      "Г ( DD )\n",
      "The experimental data for the radiative decay of the vector\n",
      "meson D*+ in Particle Data Group ( PDG ) [ 71 ] is Total ( D*+) =\n",
      "83.4 ± 1.8 keV , and ( D→D+y ) ≈ 1.6 ± 0.4 % . The de -\n",
      "( D )\n",
      "cay widths for D+0 and D are given as rotal ( D* ) < 2.1\n",
      "MeV , D ≈ 35.3 ± 0.9 % , and Frotal ( D ) < 1.9 MeV ,\n",
      "T ( D→Day ) ( D ) ( 93.5 ± 0.7 ) % , respectively . However , more exact\n",
      "values about these decays have not been determined yet . For\n",
      "the mesons B*( 0 ) and B , the radiative decays as their main\n",
      "decay channels have not been determined in experiment . In\n",
      "general , more exact experimental results and theoretical pre -\n",
      "dictions need to be provided , which can help us to understand\n",
      "the nature of the mesons and test the theoretical models .\n",
      "The layout of this paper is as follows , After introduction in\n",
      "Sec . I , the radiative decays of the vector heavy - light mesons\n",
      "are analyzed in the framework of SM in Sec . II , and the elec -\n",
      "tromagnetic form factor is introduced . In Sec . III , we sys -\n",
      "tematically analyze the electromagnetic form factors of vec -\n",
      "tor heavy - light meson to pseudoscalar heavy - light meson by\n",
      "the three - point QCDSR , where the contributions of perturba -\n",
      "tive part and vacuum condensate including ( qq ) , < qgGq > ,\n",
      "< gG² > , ( f³G³ ) and ( qq ) < gG2 > are considered in QCD side .\n",
      "Sec . IV is employed to present the numerical results and dis -\n",
      "cussions . Sec . V is reserved as conclusions . Some important\n",
      "figures are shown in Appendix .\n",
      "==End of OCR for page 2==\n"
     ]
    }
   ],
   "source": [
    "import google\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key='AIzaSyAIWLIxKL1yCk5Skw1WzNZHvPRyK5jhd6g')\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "fpath = \"C:\\\\Users\\\\sselva\\\\Downloads\\\\testddoc1.pdf\"\n",
    "with open(fpath, \"rb\") as f:\n",
    "    sample_pdf = genai.upload_file(f, mime_type=\"application/pdf\")\n",
    "response = model.generate_content([\"extract all the text word by word from this pdf file.\", sample_pdf])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import pprint\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import docx  # For DOCX file processing\n",
    "\n",
    "# Define AWS credentials and setup session\n",
    "AWS_ACCESS_KEY_ID = \"AKIAS54AKEOJBF7WM5WL\"\n",
    "AWS_SECRET_ACCESS_KEY = \"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\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "\n",
    "boto3.setup_default_session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=REGION_NAME\n",
    ")\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=REGION_NAME)\n",
    "\n",
    "# Function to encode image to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Utility function to extract images from PDF pages\n",
    "def extract_pages_as_images(pdf_path, output_folder):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page_number in range(len(doc)):\n",
    "        page = doc.load_page(page_number)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        for image_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_filename = f\"{output_folder}/page_{page_number + 1}_image_{image_index + 1}.{image_ext}\"\n",
    "            with open(image_filename, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "            images.append(image_filename)  # Collect image paths\n",
    "    return images\n",
    "\n",
    "# Utility function to extract text from DOCX files\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    text_content = [paragraph.text for paragraph in doc.paragraphs]\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "# Function to perform OCR using Claude\n",
    "def perform_ocr_with_claude(image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    # Define the request payload for Bedrock\n",
    "    payload = {\n",
    "        \"modelId\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"contentType\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "        \"body\": json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/jpeg\",\n",
    "                                \"data\": base64_image\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Perform OCR on the image and provide the extracted text.\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    }\n",
    "\n",
    "    # Make the API call to Bedrock\n",
    "    response = bedrock_client.invoke_model(**payload)\n",
    "    response_content = response['body'].read().decode('utf-8')\n",
    "    response_json = json.loads(response_content)\n",
    "    \n",
    "    return response_json['content'][0]['text']\n",
    "\n",
    "# Main function to process files and perform OCR using Claude\n",
    "def process_file_with_claude(file_path, output_folder, verbose=False):\n",
    "    file_extension = os.path.splitext(file_path)[-1].lower()\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    ocr_results = []\n",
    "\n",
    "    if file_extension == \".pdf\":\n",
    "        # Extract images from PDF and perform OCR\n",
    "        images = extract_pages_as_images(file_path, output_folder)\n",
    "        for image_path in images:\n",
    "            extracted_text = perform_ocr_with_claude(image_path)\n",
    "            ocr_results.append({\"text\": extracted_text})\n",
    "\n",
    "    elif file_extension == \".docx\":\n",
    "        docx_text = extract_text_from_docx(file_path)\n",
    "        ocr_results.append({\"text\": docx_text})\n",
    "\n",
    "        json_output_path = os.path.join(output_folder, \"docx_output.json\")\n",
    "        with open(json_output_path, \"w\") as json_file:\n",
    "            json.dump({\"text\": docx_text}, json_file, indent=4)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Processing complete. Cleaned up temporary files.\")\n",
    "\n",
    "    return ocr_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Cleaned up temporary files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': \"The extracted text from the image is:\\n\\nTesseract at UB Mannheim\\n\\nThe Mannheim University Library (UB Mannheim) uses Tesseract to perform text recognition (OCR = optical character recognition) for historical German newspapers (Allgemeine Preußische Staatszeitung, Deutscher Reichsanzeiger). The latest results with text from more than 700000 pages are available online.\\n\\nTesseract installer for Windows\\n\\nNormally we run Tesseract on Debian GNU Linux, but there was also the need for a Windows version. That's why we have built a Tesseract installer for Windows.\\n\\nWARNING: Tesseract should be either installed in the directory which is suggested during the installation or in a new directory. The uninstaller removes the whole installation directory. If you installed Tesseract in an existing directory, that directory will be removed with all its subdirectories and files.\\n\\nThe latest installers can be downloaded here:\\n- tesseract-ocr-w64-setup-5.4.0.20240606.exe (64 bit)\\n\\nThere are also older versions for 32 and 64 bit Windows available.\\n\\nIn addition, we also provide documentation which was generated by Doxygen.\"},\n",
       " {'text': 'The text extracted from the image is as follows:\\n\\nSUBJECT IN FOCUS: Origin of the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), the virus causing COVID-19\\n\\nThe first human cases of COVID-19, the disease caused by the novel coronavirus causing COVID-19, subsequently named SARS-CoV-2 were first reported by officials in Wuhan City, China, in December 2019. Retrospective investigations by Chinese authorities have identified human cases with onset of symptoms in early December 2019.\\n\\nWhile some of the earliest known cases had a link to a wholesale food market in Wuhan, some did not. Many of the initial patients were either stall owners, market employees, or regular visitors to this market. Environmental samples taken from this market in December 2019 tested positive for SARS-CoV-2, further suggesting that the market in Wuhan City was the source of this outbreak or played a role in the initial amplification of the outbreak. The market was closed on 1 January 2020.\\n\\nSARS-CoV-2 was identified in early January and its genetic sequence shared publicly on 11-12 January. The full genetic sequence of SARS-CoV-2 from the early human cases and the sequences of many other virus isolated from human cases from China and all over the world since then show that SARS-CoV-2 has an ecological origin in bat populations. All available evidence to date suggests that the virus has a natural animal origin and is not a manipulated or constructed virus. Many researchers have been able to look at the genomic features of SARS-CoV-2 and have found that evidence does not support that SARS-CoV-2 is a laboratory construct. If it were a constructed virus, its genomic sequence would show a mix of known elements. This is not the case.'},\n",
       " {'text': 'The extracted text from the image is as follows:\\n\\nI. INTRODUCTION\\n\\nAs one of the most interesting research areas in particle physics, the experimental and theoretical investigations of the heavy flavor hadrons have been developing rapidly. This is due to their key roles in testing the understanding of QCD long-distance dynamics, and the determination of the fundamental parameters of the standard model (SM). Especially, the decays of heavy flavor hadrons is an important and interesting research topic for testing the SM and finding new physics beyond the SM. These decay processes can be classified into leptonic, semileptonic and nonleptonic decays which involve both electroweak and strong interactions. However, because the experimental data about the heavy hadrons is very difficult because the QCD is non-perturbative in low energy regions.\\n\\nTherefore, many phenomenological approaches[1-34] have been employed to analyze the decay processes of the heavy flavor hadrons.\\n\\nThe QCD sum rules (QCDSR) is one of the most powerful non-perturbative approaches, and has been widely used to analyze the mass spectra and the decay behavior of hadrons [35-50]. In recent years, some talks were carried out by three-point QCDSR, such as the analysis of electroweak and electromagnetic form factors [12, 51-57], and the strong coupling constants [58-68]. These parameters are very important to analyze the decay process of hadrons. In our previous work, we employed the three-point QCDSR, and the corresponding radiative decay B*→Bγ was calculated[69]. The relative decay width, momentum dependent strong coupling constant G(B*Bγ)(q²), the electromagnetic constant G(B*B)(q²) were obtained. According to the framework of SM, the electromagnetic constant G(B*B)(q²) was obtained by setting q² = 0. And the radiative decay width for D*→Dγ was analyzed.'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_file_with_claude(\"C:\\\\Users\\\\sselva\\\\Downloads\\\\testddoc1.pdf\",\"uploads\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "APIKey = \"6527847513f041f1b7b7a30ecf778300\"\n",
    "Endpoint = \"https://pstestopenaidply-3wxqngpadhki4.openai.azure.com/\"\n",
    "Deployment = \"pstestopenaidply-3wxqngpadhki4\"\n",
    "version = \"2024-05-01-preview\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=.3,\n",
    "    azure_endpoint=Endpoint,\n",
    "    api_key=APIKey,\n",
    "    deployment_name=Deployment,\n",
    "    openai_api_version=version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = \"Create or Delete eGroup\\nCreate eGroup\\nCheck approval matrix...\"\n",
    "doc_context = [(extracted_text, 0.07996448453907072)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sselva\\AppData\\Local\\Temp\\ipykernel_5012\\944523854.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"qwen2.5:1.5b\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm=Ollama(model=\"qwen2.5:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: 73c1da94********************8ef0. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_file_with_gpt_vision\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43msselva\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtestddoc1.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muploads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 86\u001b[0m, in \u001b[0;36mprocess_file_with_gpt_vision\u001b[1;34m(file_path, output_folder, verbose)\u001b[0m\n\u001b[0;32m     84\u001b[0m     images \u001b[38;5;241m=\u001b[39m extract_pages_as_images(file_path, output_folder)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m---> 86\u001b[0m         extracted_text \u001b[38;5;241m=\u001b[39m \u001b[43mperform_ocr_with_gpt_vision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m         ocr_results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: extracted_text})\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_extension \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36mperform_ocr_with_gpt_vision\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_ocr_with_gpt_vision\u001b[39m(image_path):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Construct GPT Vision request\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     base64image \u001b[38;5;241m=\u001b[39m encode_image(image_path)\n\u001b[1;32m---> 48\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-vision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease extract the text from this image.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata:image/jpeg;base64,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase64image\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Local image path\u001b[39;49;00m\n\u001b[0;32m     62\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Process response and extract text\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo text extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\openai\\_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sselva\\Downloads\\florence2\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 73c1da94********************8ef0. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "process_file_with_gpt_vision(\"C:\\\\Users\\\\sselva\\\\Downloads\\\\testddoc1.pdf\", \"uploads\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import configparser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.properties')\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config['google']['api_key']\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
